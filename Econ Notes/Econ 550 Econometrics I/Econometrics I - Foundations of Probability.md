---
aliases:
creation date: Friday, September 2nd 2022, 5:09 pm
date updated: Tuesday, October 11th 2022, 10:40 am

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#types/classes/stats/theory'
- '#topics/methods'
---

# [[Econometrics I - Foundations of Probability]]

## Basic Concepts

An event is some subset of the possible outcomes. 

What is the probability of an event? There are many definitions, which are often unsatisfactory. We will not commit to one, but we will give axioms that create some consistency. 

The most common definition is the <font color=gree>frequentist</font>. In this case, probability is the *relative* frequency of an outcome when an experiment is repeated many times, in the limit. 
- But what does it mean to repeat an experiment? We need to invoke some idea of independence of successive trials. Why bring another concept into it?
- More troublingly, we want to talk about probability for events without repeated experiments. It would be nice to set up our probability definitions without recourse to multiverse theory. 

There is also a <font color=gree>subjectivist</font> approach: each person has their own definitions for whatever. 

### Formal Definitons

- We'll start formalizing with the <font color=gree>outcome space</font>. The outcome space - sometimes called the *universal set* - is written $\Omega$. It contains elements $\omega$ which represent all possible outcomes. 
	- This can include things that never happen! 
	- If flipping a coin, our outcome space could be heads or tails - or heads, tails, and the coin is abducted by passing aliens.
- An <font color=gree>event</font> $A$ is some subset of $\Omega$ - a collection of $\omega$s. $A$ can be the null set. 
- $A^c$ is the complement of $A$, $\{\omega \in \Omega : \omega \notin A \}$. 
	- The complement is defined relative to $\Omega$. So the complement of "rolling a 1 or a 2 on a die" could be "rolling 3/4/5/6", if $\Omega$ is 'what can happen when we roll a dice'. But it could include many more things if $\Omega$ is "things that can happen on a craps table". 
- We can define the <font color=gree>union</font> $A \cup B$ as 'all outcomes in either $A$ or $B$; we can also write $\bigcup_{j=1}^J A_j$ for a sequence of sets. 
- Similarly with the <font color=gree>intersection</font> $A \cap B$, and $\bigcap_{j=1}^J A_j$. 
- <font color=gree>De Morgan's Laws</font> are direct consequences of the above definitions. 
	- $(\bigcup A_j)^c = \bigcap A_j^c$
	- $(\bigcap A_j)^c = \bigcup A_j^c$

```ad-info
title: De Morgan's Laws in Logic

It might be helpful to consider the analogous applications of De Morgan's Laws in logic. Suppose I have a statement 'Both $A$ and $B$ are true', which we write $A \land B$. The opposite of that is: Either $A$ or $B$ is not true - $\neg A \lor \neg B$. 

Similarly, I can say $A$ or $B$ -  $A \lor B$ - is true. The opposite of that is neither $A$ nor $B$ is true - i.e. both $A$ and $B$ are false, $\neg A \land \neg B$.

```

## $\sigma$-Fields

A somewhat technical concept we will need to use later.

A $\sigma$-field, also called a $\sigma$-algebra, iss a class of subsets of some set with certain properties. In this case, some collection of subsets of $\Omega$. 

Let $\mathcal{F}$ - 'script-F' - be a $\sigma$-field. Then: 
- $\Omega \in \mathcal{F}$
- If $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$. Call this 'closure under complements'.
- If $A_1, A_2, \ldots \in \mathcal F$ then $\bigcup_{i=1}^\infty A_i \in \mathcal F$. Closure under countable unions. We'll return to the rationale for this, 

The collection of sets for which we can compute a probability will have to be a $\sigma$-field. 

When $\Omega$ is countable, $\Omega$ is a $\sigma$-field. Not true when $\Omega$ is uncountable - likely won't matter in practiec. 

Consequences of these definitions:
1) $\emptyset \in \mathcal F$. Follows from $1$ and $2$. 
2) If $A_1, A_2, \ldots \in \mathcal F$, then $\cap^\infty A_i \in \mathcal F$. 
	- We know $\cup^\infty A_i \in \mathcal F$; $\cup^\infty A_i^c \in \mathcal F$; so by De Morgan's Laws, $\cap^\infty A_i \in \mathcal F$. 

Can weaken property 3 to be closure under *finite* unions. This is a field/algebra. 

Suppose $\mathcal C$ - script-C - is a collection of subsets of $\Omega$. We can generate some $\sigma(\mathcal C)$ that is the smallest $\sigma$-field that contains $\mathcal C$. By definition, $\sigma(\mathcal C)$ is $\cap_{\tau \in \mathcal T} \mathcal C_\tau$, where $\{ \mathcal C_\tau : \tau \in \mathcal T\}$ is the collection of all $\sigma$-fields that contain $\mathcal C$. It can be shown that this is a $\sigma$-field. 

The most commonly used $\sigma$-field is the Borel $\sigma$-field: the field generated by all open sets in some topology. Most often the Borel $\sigma$-field on $\R$. 

In practice we will probably not worry too much about $\sigma$-fields.

## Probability

### The Kolmogorov Axioms

A probability space is simply some collection $(\Omega, \mathcal F, P)$. $\Omega$ is an outcome space, $\mathcal F$ is a $\sigma$-field, and $P$ is a probability measure. A probability measure satisfies the following properties (the Kolmogorov axioms): 

Let $P$ be some function from $\mathcal F$ to $\R$. Then:
1) $P(A) \geq 0 \; \forall A \in \mathcal F$. 
2) $P(\Omega) = 1$
3) Countable additivity: If $A_1, A_2, \ldots$ are a countable collection of disjoint events, then $P(\bigcup A_i) = \sum P(A_i)$. 

These correspond to basic intuitions from the frequentist understanding. 1) corresponds to the fact that you can't have negative frequency; 2) describes the *relative* part of relative frequency. 3) in a weaker form - finite additivity - is a clearly desirable relative-frequency property. 

Why countable rather than finite? We will come back to that. 

Note: it's possible to have a $\sigma$-field on which no $P$ can be defined. 

``` ad-info
title: Basic Intro to Measures 

Probability is a specific case of a measure. A measure $\mu$ only has to fulfil properties: 
1) $\mu(A) \geq 0$
2) $\mu(\emptyset) = 0$
3) $\mu(\bigcup A_i) = \sum \mu(A_i)$ where $A_i$ are countable and disjoint. 

A commonly used measure is the Lebesgue measure, which corresponds to length in $\R^1$, area in $\R^2$, volume in $\R^3$, and so on. This is a uniform measure - it puts no weights on particular points. 

The counting measure is a measure commonly on $\mathbb Z$, but can be on any countable set. It's how many things there are. 
```


### Direct Implications of the Axioms

1) $P(B) = P(B \cap A) + P(B \cap A^c)$
	- The "Law of Total Probability". We know this from countable additivity.
2) $P(A^c) = 1 - P(A)$.
	- since $P(\Omega) = 1$, and $\Omega = A \cup A^c$, and $A, A^c$ are disjoint, then $P(A) + P(A^c) = P(\Omega) = 1$. 
3) $P(\emptyset) = 0$.
	- Since $\Omega = \Omega \cup \emptyset$, $P(\Omega) = P(\Omega) + P(\emptyset) = 1 + P(\emptyset)$. 
4) If $A \subseteq B$, $P(A) \leq P(B)$ 
	- We can write $P(A) = P(A \cap B) + P(A \cap B^c)$, given implication 1). But from the subset we know that $A \cap B = B$. So $P(A) + P(B) +$ some nonnegative quantity. 
5) $0 \leq P(A) \leq 1$. Follows from 4), since $A \subseteq \Omega$ and $P(\Omega) = 1$. 
6) $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. 
	- LTP tells us that $P(B) = P(A \cap B) + P(A^c \cap B)$. So $P(A^c \cap B) = P(B) - P(A \cap B)$
	- We can write $A \cup B = A \cup (A^c \cap B)$
	- So $P(A \cup B) = P(A) + P(A^c \cap B) = P(A) + P(B) - P(A \cap B)$. 


### Intuition for the Axioms

So why these particular axioms? 
- Why countable, rather than finite additivity? Suppose our experiment is infinite tosses of a fair coin; thus the outcome space is any infinite sequence of $\{0,1\}$. 
	- We can easily create finite events: the outcome of the first $n$ tosses is more than $n/2$ heads, etc. So then we could have $\mathcal F$ be all finite-number-of-tosses events. 
	- But we might want to describe non-finite events. For instance, $\lim_{n \to \infty} \frac{\sum \text{heads}}{n} > 1/2$ might be an assessment of fairness. 

Suppose we have axioms 1 and 2 and finite additivity. Is there a Pr distribution that makes sense over power set of $\Omega$, rather than just $\mathcal F$? Yes, but we don't have uniqueness. Adding the countable additivity means that no such distribution exists - there are non-measurable sets (sets with no measure) that generate different probabilities under the same distribution. So we have to restrict ourselves to the $\sigma$-fields, which maps to common-sense desiderata. 

There are 'extension theorems' that let you take $P: \mathcal G \to \R$ and extend it to $P: \sigma(\mathcal G) \to R$. 

Example: The Lebesgue measure on $[0,1]$ is already a probability measure. Suppose we have $\mathcal F$ as all finite unions of disjoint intervals. Then the probability is just the sum of the interval lengths. 



## Conditional Probability

Suppose you throw a dart at a board with a Venn Diagram on it. You're blindfolded and you want to know whether your dart landed in $A$. 

![[Econometrics II - Working with Random Variables 2022-09-15 18.13.21.excalidraw|300]]

I tell you the dart landed in $B$. What can I say about whether it landed on $A$?

This is a <font color=gree>conditional probability</font>. We can see that the probability should scale with the probability of $A \cap B$, so we can say $P(A) = kP(A\cap B)$. We want $P(B|B) = 1$; so we can then see that $P(B|B) = 1 = kP(B\cap B) = k P(B)$ and so $k = 1/P(B)$. 

This gives us the formula that 

$$ P(A|B) = P(A\cap B)/ P(B)$$

#### The Conditional Probabilty Measure
This gives us a new probability space, $(\Omega, \mathcal F, P( \cdot | B))$. Is $P(\cdot|B)$ a valid distribution?
- It's nonnegative, since both $P(B)$ and $P(A|B)$ are nonnegative (since $P$ is a distribution). 
- The probability of $\Omega$ is $1$: $P(\Omega|B) = P(\Omega \cap B)/P(B)$

Countable additivity is always fun... 
$$P(\bigcup A_i |B ) = P\left[ \left(\bigcup A_i\right) \cap B \right]/ P(B) $$
By the distributive property of intersections this becomes 

$$P\left[ \bigcup (A_i \cap B\right)] / P(B) $$
Since $A_i$ are disjoint, $A_i \cap B$ are disjoint, and in $\mathcal F$. So by additivity of $P$ we can write this as $\sum_i P(A_i \cap B)/ P(B) = \sum_i P(A_i |B)$. 

#### Conditional Independence

To say $C$ is <font color=gree>independent</font> of $B$ is to say that knowing $B$ gives us nothing about $C$. Disjoint events are not independent! Ex. $B$, $B^c$ are about as dependent as it gets...

Event $A$ is independent of $B$ - written also $A \indep B$ - if $P(A|B) = P(A)$. 

We can then say $P(A\cap B)/P(B) = P(A)$, so $P(A \cap B) = P(A)P(B)$. Both are used as independence definitions. 

$\emptyset$ and $\Omega$ are both independent of everything. 

We can extend this: If $A \indep B$ then 
- $A \indep B^c$
- $A^c \indep B$
- $A^c \indep B^c$

Proof: #status/section/ðŸš§ 

Extend this to random variables: $X,Y$ are independent if for all sets $A \in \Omega_X$ and $B \in \Omega_Y$, the events $X \in A$ and $Y \in B$ are independent. We'll come back to this. 




![[2002.1 Probability#Bayes' Rule]]
