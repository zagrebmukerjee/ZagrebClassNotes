---
aliases:
creation date: Friday, September 2nd 2022, 5:09 pm
date updated: Wednesday, September 7th 2022, 2:58 pm

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#types/classes/stats/theory'
- '#topics/methods'
---

# [[Econometrics I - Foundations of Probability]]

## Basic Concepts

An event is some subset of the possible outcomes. 

What is the probability of an event? There are many definitions, which are often unsatisfactory. We will not commit to one, but we will give axioms that create some consistency. 

The most common definition is the <font color=gree>frequentist</font>. In this case, probability is the *relative* frequency of an outcome when an experiment is repeated many times, in the limit. 
- But what does it mean to repeat an experiment? We need to invoke some idea of independence of successive trials. Why bring another concept into it?
- More troublingly, we want to talk about probability for events without repeated experiments. It would be nice to set up our probability definitions without recourse to multiverse theory. 

There is also a <font color=gree>subjectivist</font> approach: each person has their own definitions for whatever. 

### Formal Definitons

- We'll start formalizing with the <font color=gree>outcome space</font>. The outcome space - sometimes called the *universal set* - is written $\Omega$. It contains elements $\omega$ which represent all possible outcomes. 
	- This can include things that never happen! 
	- If flipping a coin, our outcome space could be heads or tails - or heads, tails, and the coin is abducted by passing aliens.
- An <font color=gree>event</font> $A$ is some subset of $\Omega$ - a collection of $\omega$s. $A$ can be the null set. 
- $A^c$ is the complement of $A$, $\{\omega \in \Omega : \omega \notin A \}$. 
	- The complement is defined relative to $\Omega$. So the complement of "rolling a 1 or a 2 on a die" could be "rolling 3/4/5/6", if $\Omega$ is 'what can happen when we roll a dice'. But it could include many more things if $\Omega$ is "things that can happen on a craps table". 
- We can define the <font color=gree>union</font> $A \cup B$ as 'all outcomes in either $A$ or $B$; we can also write $\bigcup_{j=1}^J A_j$ for a sequence of sets. 
- Similarly with the <font color=gree>intersection</font> $A \cap B$, and $\bigcap_{j=1}^J A_j$. 
- <font color=gree>De Morgan's Laws</font> are direct consequences of the above definitions. 
	- $(\bigcup A_j)^c = \bigcap A_j^c$
	- $(\bigcap A_j)^c = \bigcup A_j^c$

```ad-info
title: De Morgan's Laws in Logic

It might be helpful to consider the analogous applications of De Morgan's Laws in logic. Suppose I have a statement 'Both $A$ and $B$ are true', which we write $A \land B$. The opposite of that is: Either $A$ or $B$ is not true - $\neg A \lor \neg B$. 

Similarly, I can say $A$ or $B$ -  $A \lor B$ - is true. The opposite of that is neither $A$ nor $B$ is true - i.e. both $A$ and $B$ are false, $\neg A \land \neg B$.

```

## $\sigma$-Fields

A somewhat technical concept we will need to use later.

A $\sigma$-field, also called a $\sigma$-algebra, iss a class of subsets of some set with certain properties. In this case, some collection of subsets of $\Omega$. 

Let $\mathcal{F}$ - 'script-F' - be a $\sigma$-field. Then: 
- $\Omega \in \mathcal{F}$
- If $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$. Call this 'closure under complements'.
- If $A_1, A_2, \ldots \in \mathcal F$ then $\bigcup_{i=1}^\infty A_i \in \mathcal F$. Closure under countable unions. We'll return to the rationale for this, 

The collection of sets for which we can compute a probability will have to be a $\sigma$-field. 

When $\Omega$ is countable, $\Omega$ is a $\sigma$-field. Not true when $\Omega$ is uncountable - likely won't matter in practiec. 

Consequences of these definitions:
1) $\emptyset \in \mathcal F$. Follows from $1$ and $2$. 
2) If $A_1, A_2, \ldots \in \mathcal F$, then $\cap^\infty A_i \in \mathcal F$. 
	- We know $\cup^\infty A_i \in \mathcal F$; $\cup^\infty A_i^c \in \mathcal F$; so by De Morgan's Laws, $\cap^\infty A_i \in \mathcal F$. 

Can weaken property 3 to be closure under *finite* unions. This is a field/algebra. 

Suppose $\mathcal C$ - script-C - is a collection of subsets of $\Omega$. We can generate some $\sigma(\mathcal C)$ that is the smallest $\sigma$-field that contains $\mathcal C$. By definition, $\sigma(\mathcal C)$ is $\cap_{\tau \in \mathcal T} \mathcal C_\tau$, where $\{ \mathcal C_\tau : \tau \in \mathcal T\}$ is the collection of all $\sigma$-fields that contain $\mathcal C$. It can be shown that this is a $\sigma$-field. 

The most commonly used $\sigma$-field is the Borel $\sigma$-field: the field generated by all open sets in some topology. Most often the Borel $\sigma$-field on $\R$. 

In practice we will probably not worry too much about $\sigma$-fields.

## Probability

### The Kolmogorov Axioms

A probability space is simply some collection $(\Omega, \mathcal F, P)$. $\Omega$ is an outcome space, $\mathcal F$ is a $\sigma$-field, and $P$ is a probability measure. A probability measure satisfies the following properties (the Kolmogorov axioms): 

Let $P$ be some function from $\mathcal F$ to $\R$. Then:
1) $P(A) \geq 0 \; \forall A \in \mathcal F$. 
2) $P(\Omega) = 1$
3) Countable additivity: If $A_1, A_2, \ldots$ are a countable collection of disjoint events, then $P(\bigcup A_i) = \sum P(A_i)$. 

These correspond to basic intuitions from the frequentist understanding. 1) corresponds to the fact that you can't have negative frequency; 2) describes the *relative* part of relative frequency. 3) in a weaker form - finite additivity - is a clearly desirable relative-frequency property. 

Why countable rather than finite? We will come back to that. 

Note: it's possible to have a $\sigma$-field on which no $P$ can be defined. 

``` ad-info
title: Basic Intro to Measures 

Probability is a specific case of a measure. A measure $\mu$ only has to fulfil properties: 
1) $\mu(A) \geq 0$
2) $\mu(\emptyset) = 0$
3) $\mu(\bigcup A_i) = \sum \mu(A_i)$ where $A_i$ are countable and disjoint. 

A commonly used measure is the Lebesgue measure, which corresponds to length in $\R^1$, area in $\R^2$, volume in $\R^3$, and so on. This is a uniform measure - it puts no weights on particular points. 

The counting measure is a measure commonly on $\mathbb Z$, but can be on any countable set. It's how many things there are. 
```


### Direct Implications of the Axioms

1) $P(B) = P(B \cap A) + P(B \cap A^c)$
	- The "Law of Total Probability". We know this from countable additivity.
2) $P(A^c) = 1 - P(A)$.
	- since $P(\Omega) = 1$, and $\Omega = A \cup A^c$, and $A, A^c$ are disjoint, then $P(A) + P(A^c) = P(\Omega) = 1$. 
3) $P(\emptyset) = 0$.
	- Since $\Omega = \Omega \cup \emptyset$, $P(\Omega) = P(\Omega) + P(\emptyset) = 1 + P(\emptyset)$. 
4) If $A \subseteq B$, $P(A) \leq P(B)$ 
	- We can write $P(A) = P(A \cap B) + P(A \cap B^c)$, given implication 1). But from the subset we know that $A \cap B = B$. So $P(A) + P(B) +$ some nonnegative quantity. 
5) $0 \leq P(A) \leq 1$. Follows from 4), since $A \subseteq \Omega$ and $P(\Omega) = 1$. 
6) $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. 
	- LTP tells us that $P(B) = P(A \cap B) + P(A^c \cap B)$. So $P(A^c \cap B) = P(B) - P(A \cap B)$
	- We can write $A \cup B = A \cup (A^c \cap B)$
	- So $P(A \cup B) = P(A) + P(A^c \cap B) = P(A) + P(B) - P(A \cap B)$. 


### Intuition for the Axioms

So why these particular axioms? 
- Why countable, rather than finite additivity? Suppose our experiment is infinite tosses of a fair coin; thus the outcome space is any infinite sequence of $\{0,1\}$. 
	- We can easily create finite events: the outcome of the first $n$ tosses is more than $n/2$ heads, etc. So then we could have $\mathcal F$ be all finite-number-of-tosses events. 
	- But we might want to describe non-finite events. For instance, $\lim_{n \to \infty} \frac{\sum \text{heads}}{n} > 1/2$ might be an assessment of fairness. 

Suppose we have axioms 1 and 2 and finite additivity. Is there a Pr distribution that makes sense over power set of $\Omega$, rather than just $\mathcal F$? Yes, but we don't have uniqueness. Adding the countable additivity means that no such distribution exists - there are non-measurable sets that generate different probabilities under the same distribution. So we have to restrict ourselves to the $\sigma$-fields, which maps to common-sense desiderata. 

There are 'extension theorems' that let you take $P: \mathcal G \to \R$ and extend it to $P: \sigma(\mathcal G) \to R$. 

Example: The Lebesgue measure on $[0,1]$ is already a probability measure. Suppose we have $\mathcal F$ as all finite unions of disjoint intervals. Then the probability is just the sum of the interval lengths. 


## Random Variables

A random variable $X$ is a mapping from $\Omega$ to some $\Omega_X \subseteq \R$. Example: if we have $\Omega$ be a survey of a number of people on employment status - unemployed, FT employed, PT employed. Then $X: \Omega \to \{ 0,1 \}$ might be $X = 0$ if unemployed, $1$ otherwise. 

Note: the randomness here comes from the underlying state of the world $\Omega$. The RV is just reflecting that. 

So now we want to create a probability measure on random variables. We can do this simply: 

$$P(X \in [a,b]) = P(\{ \omega \in \Omega: X(\omega) \in [a,b] \})$$
In other words, the preimage of $[a,b]$. We can generalize this to any set $B \subseteq \Omega_X$: 
$$P(X \in B) = P(\omega \in \Omega: X(\omega) \in B)$$
We can write $P(X \in B)$. But this is just notation. The above is the real definition. 

This lets us say that at least $\Omega_X$ contains $\{ X(\omega): \omega \in \Omega \}$. This doesn't restrict us: e.g. in the unemployment event above have $\Omega_X = [0,1]$, and only have nonzero probability at the endpoints. We might want that for convenience. 

```ad-note
title: Measure Theory Conditions

We know probability only exists for subsets of $\Omega$ that are in $\mathcal F$. So we need to make sure that $\{ \omega \in \Omega: X(\omega) \in B \}$ is in $\mathcal F$. Let $\mathcal B(\Omega_X)$ be the Borel $\sigma$-field generated by $\Omega_X$. We will only assign probabilities to sets in $\mathcal B$. 

We say that $X$ is a _measurable_ function from $(\Omega, \mathcal F)$ to $(\Omega_X, \mathcal B(\Omega_X))$ if $\forall \; B \in \mathcal B$, $\{\omega \in \Omega: X(\omega) \in B\} \in \mathcal F$ (that is to say, every set in $\mathcal B$ has a preimage in $\mathcal F$). #status/section/🚧. Random variables will be measurable by definition. In practice, it's actually hard to construct nonmeasurable functions. So this is not a major concern. 

```

We can also write $P(\{ \omega \in \Omega: X(\omega) \in B\})$ as $P_X(B)$. Thus we can say $X$ and the distribution of $\Omega$ <font color=gree>induces</font> some distribution.

$P_x$ satisfies the probability axioms: 
- Non-negativity: Each $B$ has a preimage with $0$ or more $\omega$, meaning that non-negativity follows from non-negativity of $P$ on $\Omega$. 
- $P_X(\Omega_X) = 1$: $P_X(\Omega_X)$ is $\{ \omega \in \Omega : X(\omega) \in \Omega_X \}$. But $\Omega_X$ is simply all those points that have some preimage $\omega$ in $\Omega$. Thus, $P_X(\Omega_X) = P(\Omega) = 1$. 
- Countable additivity of disjoint events. Left as exercise

This creates a probability space: $(\Omega_X, \mathcal B(\Omega_X), P_X)$. 


```ad-example
title: Countable Additivity of $P_X$



```

Suppose $B_1,B_2, \ldots$ are disjoint in $\mathcal B$. 



