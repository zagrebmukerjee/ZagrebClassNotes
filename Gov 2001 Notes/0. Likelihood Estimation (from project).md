---
date updated: 2021-06-13 15:52

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
---

#### [[0. Probability and Likelihood Estimation (from project)]]
Part of [[@Gov 2001 Index]]

##### Probability Distributions

Probability distributions are models that turn parameters $p$ into values of a random variable $y$. They can tell us the probability that $y$ takes on particular values given $p$. 

For a discrete case, take a fair coin flip. This has no parameters; the probability distribution looks like this:

$$P(y|p) = f(y) = \begin{cases} \frac{1}{6}& \text{if} \: y \in \{1, \ldots, 6\}\\
\\
0 &\text{otherwise}
\end{cases} $$

The function $f$ is called a _Probability Mass Function_ (PMF). For any $x$, $f(x)$ gives the probability that $y = x$. 
 
 PMFs must only take on values between $0$ and $1$, and the sum of the PMF across all $x$ must be $1$.

##### Continuous Distribution

Distributions can also be continuous. In this case, there is no PMF. Our function of interest $f(y|p)$ then is not a PMF - it is instead a _Probability Distribution Function_ (PDF). The indefinite integral of a PDF must be $1$.

In this case, $f(x)$ is a density - not interpretable as $x$, since by the properties of integration, the probability of any one $x$ is zero. 

To get probabilities, we can instead construct something like this:

$$P(x|x \in [a,b]) = \int_a^b f(x) dx$$

The area under the curve for any particular subset of $\mathbb{R}^1$ tells us the probability $x$ lies in that subset. 

##### Simple Case: Bernoulli

The Bernoulli Distribution describes a situation like an unfair coin flip. It takes on values $0$ or $1$ - sometimes called failure or success. 

It has one parameter $\pi$, defined as the probability of success. So the PMF looks like this:

$$f_{bern} = \begin{cases}\pi & \text{if} \; x= 1 \\ 1-\pi & \text{if}  \; x = 0 \\
0 & \text{otherwise} \end{cases}$$



##### Likelihood Estimation 


Likelihood Estimation is a theory of inference

The idea is to compute likelihood - how likely is it that parameters $x$ generated data $y$ given model $M$?

This is (probably) proportional to the probability that $M(x) = y$ - the _likelihood function_. So we want to find the parameters $x$ such that $x$ is most likely to have generated $y$ given some assumptions about the distribution of $y|x$.

