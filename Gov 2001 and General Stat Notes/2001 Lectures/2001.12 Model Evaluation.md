---
date updated: 2021-10-31 16:26

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
---

# [[2001.12 Model Evaluation]]
Part of [[@Gov 2001 and Stats Index]]


We have our models, how do we know they are right? 
- Look creatively for observable implications and then observe them
- Look for as many lowly correlated evaluations as possible. This will pre-empt readers' and reviewers' objections.

### Out of sample forecast

Use part of your sample to fit model and then use it to predict the other parts - then compare to truth
-	compare both average values and full distribution
-	Best test sets are truly out of sample - forecast the election before it happens 
-	Model might still fail if underlying DGPs fail/the world changes


![[Pasted image 20211031163916.png]]

- Complex model - eg regression with a billion coefficients
- As you fit data solely to the training set, you will fit it better. 
	- But at some point you will stop getting the fundamental processes at work, and start overfitting the data
	- That's when blue line diverges from pink 


### Cross-Validation
- randomly select $k$ observations from your dataset, those are the test set, the rest $n-k$ are training set. Fit and evaluate. Then get another set of $k$. 
- Report your fit over the average of the test sets to lower the variance
- Since we can't wait to predict 20 more elections

### Fit in general
- Look for other observable implications - be creative! If in-sample data aren't fitting model, you may have a problem

### Regression Diagnostics
- eg. plots of residuals $e = \yhat - y$ vs X, Y, $\yhat$
- check the means, but other things too: eg. $e$ vs $\yhat$ should be normally distributed ish - check $\pm$ 1 or 2 SE's
- These might be clues to the errors
- For graphics: transform bounded variables. 
- transform homoskedastic results
- highlight key results

### ROC curves for Binary Models

- Receiver-Operator Characteristics curves
- Binary predictions require normative decision: models only create probabilities. 
- How to compare to RW output that's only 0 or 1?
	- Let $C$ be the cost of wrongly saying a 0 is 1 / the cost of wrongly saying a 1 is 0. 
	- $C$ must be chosen indepedently of data - philosophy, literature, etc. 
	- Decision theory result: choose $Y = 1$ when $\hat{\pi} > 1/(1+C)$, and $0$ otherwise.
	- If $C$ is 1 - just choose $1$ if $\hat{\pi} > .5$
	- Otherwise choose to round in one direction or another
	- Then you can compute $\%$ of 1s and 0s correctly predicted
- What if we don't know $C$? Or don't want to pick a $C$? Then
	- We can look at every $C$ and then compute $\%$ of 1s and 0s correctly predicted.
	- Plot  $\%$ of 1s and 0s correctly predicted for each $C$ - this will create a 'production frontier'
	-  Overlay these curves for separate models. if one curve is above the other for every $C$, then it dominates, regardless of choices of $C$. So can avoid normative decision.

### Calibration for Binary Models

- sort the estimated probabilities into bins of eg $0.1$ width.
- in each bin compute: mean prediction (parametric), and average fraction of 1s (non-parametric). 
- Plot them against each other and look for systematic deviation from 45$^\circ$ line - the further the line, the higher the error. 