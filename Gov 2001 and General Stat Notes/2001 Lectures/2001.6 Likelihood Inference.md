---
date updated: 2021-09-24 17:34

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
- '#ðŸš§'
---

# [[2001.6 Likelihood Inference]]
Part of [[@Gov 2001 and Stats Index]]


## Guiding Example: Stylized Normal

Stochastic component: 
$$ Y_i \sim f_{stn}(y_i|\mu_i)$$
Systematic component:
$$ \mu_i = \beta$$
Assumption:
$$Y_i \perp Y_j \quad \forall  \; i \neq j$$

Get the joint density:
$$P(y_1, \ldots, y_n | \mu_1, \ldots, \mu_n) = \prod_{i=1}^n (2\pi)^{-1/2}\exp(\frac{-(y_i - \mu_i)^2}{2}) $$

Reparametrize with systematic component
$$P(y|\mu) = \prod_{i=1}^n (2\pi)^{-1/2}\exp(\frac{-(y_i - \beta)^2}{2})$$

This so far is probability part - the model. 

Now we can get the likelihood model:
$$ 
\begin{aligned} 
L(\beta|y) &= k(y)\prod_{i=1}^n f_{stn}(y_i)\\
&= \prod_{i=1}^n (2\pi)^{-1/2}\exp(\frac{-(y_i - \beta)^2}{2})\\
&= \sum_{i=1}^n (-1/2)\ln(2\pi) +  \sum_{i=1}^n (-1/2)(y_i - \beta)^2\\
&\dot{=} -\frac{1}{2}\sum_{i=1}^n (y_i - \beta)^2\ \\
\end{aligned}$$

look - it's least squares! \[whyyy\]


## Aside: Curse of Dimensionality

Suppose you have 1 variable with 10 categories. How can you avoid assuming linearity: 9 or 10 dummy variables. 

Now what if you have 2 explanatory variables? you need $10^2= 100$ - oh no! If you had 10 explanatory variables? $10$ billion!!

## Maximizing Likelihood

#### Analytically
How to do it? Can sometimes do it analytically - differentiate and get an FOC; solve for $\theta$ and get $\thetahat$

Make sure to check second order condition to make sure u are maximizing and not minimizing. So 2nd derivative has to be negative (concave)

#### With Numbers

make the computer do it!
use optim in R


### Finite Sample Properties of the MLE

How good is it? It maximizes the log likelihood - which is what we wanted to do!

but it could still be not that good

- MLE is MVUE if MVUE exists
	- Unbiasedness: $E(\thetahat) = \theta$
	- Efficiency - minimum variance
		- Variance to be minimized: $V(\thetahat)$
		- Ex: $V(\bar{Y}) = (1/n^2)\sum V(Y_i) = \sigma^2/n$
		- Efficiency: define $\thetahat$ to minimize $V(\thetahat)$ conditional on unbiasedness
	- If there is an MVUE, ML will find it
	- If not, ML will do pretty well :)
- Invariant to reparametrization
	- Both are MLEs: estimate $\sigma^2$ with $\hat{\sigma}^2$ OR estimate $\sigma$ with $\hat{\sigma}$ and square it
	- NOT true of other methods of inference. eg. $E(1/\ybar) \neq 1/E(\ybar)$
- Invariance to sampling plans
	- it's OK to look at the results when deciding how much data to collect


### Asymptotic Properties of the MLE

- LLN: Consistency. as $n \to \infty$ Sampling distribution collapses to a spike over the parameter value
- Asymptotic Normality (Central Limit Theorem): as $n \to \infty$ repeated samples of MLE converge to normality
	- For large enough $n$ we can just use the normal
- Asymptotic efficiency: as $n \to \infty$ MLE contains as much info as u can have in a point est; it is the MVUE (using the data as efficiently as u can)

