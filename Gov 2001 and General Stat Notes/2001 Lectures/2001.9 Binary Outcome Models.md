---
date updated: 2021-10-20 23:05

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
- '#ðŸš§'
---

# [[2001.9 Binary Outcome Models]]
Part of [[@Gov 2001 and Stats Index]]


### Linear Probability Model

 Most basic binary outcome: run a regression with dichotomous $Y$
 
 - Stochastic component is $Y_i \sim \text{Bernoulli}(\pi_1)$
 - Systematic component: $\pi_i = X_i\beta$
 - Independence assumption

some advantages: endogeneity, easier to correct fr measurement error
clear disadvantage: not bounded 1/0
Also quite hard to get the uncertainty right


### Logit Model

- Stochastic component: $Y_i \sim \text{Bernoulli}(\pi_1)$
- Systematic component: $\pi_i = 1/(1+\exp(-X_i\beta))$ 
	- Has the characteristic logistic function shape (escalator, S-curve)
- Independence assumption


### Functional forms

QOIs: 
- probability for a given x, or range of x's
- first differences: fix all variables, move one, see what happens (like a partial derivative) - can move more than one, but it's harder. $g(X', \betahat) - g(X, \betahat)$. In logit - difference of pis
- derivative rule of thumb... take derivative $\frac{\partial \theta}{\partial X_j} = \frac{\partial g(X, \beta)}{\partial X_j}$. Quick linear approximation
	- you can use this just to interpret some coefficient. The linear effect of $X$ on $Y$ will vary. but for a logit you can grab the biggest value - when $\pi = .5 = \betahat/4$.  For a probit, $\betahat \times 0.4$. 


### Alternative interpretations

From biology: 
- Continuous unobserved variable $Y^*$ -health, voting propensity. 
- Binary outcome that's our "observation mechanism"
- We say - if $Y^*_i > \tau$ we observe $y_i = 1$, else 0
- If only $y_i$ is observed and $y^*$ is standardized logistic, then we get the logit model mapping from $Y^*$ to observation


