---
date updated: 2021-09-12 17:43

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
- '#ðŸš§'
---

## [[2001.2 Statistical Models]]
Part of [[@Gov 2001 and Stats Index]]

A **model** is an abstract representation of something, meant to highlight the most important parts for some purpose. 

#### Notation
##### Dependent or "outcome" variable
- $Y$ is $n \times 1$ matrix
- $y_i$ is a number after we know it
- $Y_i$ is a random variable (ie before we know it)

"Dependent variable": can be a whole column or a single entry 

##### Explanatory Variable
- aka "covariates", "independent", or "exogeneous"
- $X = \{x_{ij}\}$; matrix. $n \times k$, $i$ is row and $j$ is column
- Can be a set of columns $X = \{x_1 \ldots x_k \}$
- Can be a set of rows , row $i$ is  $\{x_{i1} \ldots x_{ik} \}$

By convention $X$ is usually fixed 

##### Regression Notation

Linear regression model writes $Y_i = X_i\beta + \epsilon_i$. $X_i\beta$ is the "systematic component", and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ is the "stochastic component"

##### Generalized Regression
$Y_i \sim \mathcal{N}(\mu_i, \sigma^2)$ the stochastic component
$\mu_i = x_i\beta\quad$   the systematic component

So for observation $i$, you expect it "on average" to be $x_i \beta$, but there will be normally distributed noise around this. Same as regression notation if you remember that, given constant $c$:
$$[c + \mathcal{N}(0,\sigma^2)] \sim \mathcal{N}(c,\sigma^2)$$

Question: Is a histogram of $Y$ a test of normality?
My answer: no - it includes the $X$. Imagine $X$ is $\text{Bernoulli}(.5)$ and $\sigma = .001$. Then the histogram will be two-humped even if $Y$ is drawn normally around X

General way to think about method problems: simplest possible case (he picked mine haha)

##### Generalized to a wide variety of models
$Y_i \sim f(\theta_i, \alpha)$ is the stochastic component
$\theta_i = g(x_i, \beta)$ is the systematic component. 

$Y$ is outcome variable, $f$ is a density
$\theta_i$ is some systematic feature of the density (that varies over $i$) - can be mean, variance, whatever.
$\alpha$ is an "ancillary parameter", some feature of the density that doesn't vary over $i$. For homoskedastic regression $\alpha = \sigma^2$. 
$g$ is functional form. Represents systematic component - can be linear, logistic, w/e
$x_i$ is explanatory variable vector
$\beta$ is effect parameters

In the case of regression, $\theta_i = \mu_i$, $f$ is $\mathcal{N}$, and $g$ is $g(a,b) = ab$. 

##### Forms of Uncertainty

Estimation uncertainty: we don't know $\beta$ and $\alpha$, since we haven't sampled everything in the world. Shrinks as $n$ goes up.

Fundamental uncertainty: Our model doesn't capture everything. The rest we represent in distribution $f$. This is the stochastic component

Model dependence: Maybe our whole model is wrong

Quiz - if you know the whole model including $\alpha$ and $\beta$ is $R^2$ going to be $1$?
My answer: No - fundamental uncertainty will still get you


##### Systematic Components

Systematic component can be anything. Examples: linear, logistic,
$$V(Y_i) =  \sigma_i^2 = e^{x_i\beta}$$

We're identifying _classes_ of functional forms, and then our software/math/whatever tells us which member of that class fits - by setting $\beta$

Standard procedure: 
- use theory: assume a class of functional forms to identify potential relationships
- Use data: choose which member of the class by estimating params
- Remain uncertain: fundamental & estimation uncertainty, model dependence

Wrong model -> misspecification, possibly bias
Can still get you a (linear, logistic, quadratic...) approximation of the right model
Can be close to or far from truth


##### Stochastic Components

There are different kinds: eg normal, lognormal (bounded by zero), bernoulli
poisson - discrete, countably infinite on non negatives

##### Choosing components

If one is bounded, other is too
if stochastic is between 0 and 1, systematic must be globally nonlinear or flat (possibly locally linear)

First question you ask of any empirical paper: What is the model?

### Data Generation Processes

If we have data, we want to know where it came from. Otherwise it's not useful. Models are trying to learn about the DGP. Bern(.5) can be a model of flipping a fair coin. 

This is a place where theory and data can inform each other

Always useful to think about

##### Simulation 

It's great for:
1) understanding a DGP
2) Solving probability problems
3) Evaluate estimators
	1) eg: you can create a probability model, draw data from that world, apply your estimator, and see if you get the (known) parameter back. 
4) Calculate properties of densities 
5) Compute and visualize QOIs
6) Get the right answer easier than math


It's like a survey - except instead of sampling the population, you can sample from your model. And it's free. 

ex: simulation to solve the monty hall probelm