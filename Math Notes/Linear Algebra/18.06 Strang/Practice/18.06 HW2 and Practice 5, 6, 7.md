---
date updated: 2022-04-20 19:17

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#types/classes/math/linalg'
---
[[18.06 Strang]]

## Practice 5

![[Pasted image 20220423104057.png]]
a) 
$$P = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix} \to P^2 = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0\end{bmatrix} \to P^3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{bmatrix} $$
b)
$$\widehat P = \begin{bmatrix} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\\end{bmatrix} \to \widehat P^2 = \begin{bmatrix} 0 & 0 & 1 & 0\\ 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 1\\\end{bmatrix} \to \widehat P^4 = \begin{bmatrix} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\\end{bmatrix}$$
![[Pasted image 20220423105136.png]]

a) Each nonzero element in a $4\times4$ upper-triangular matrix: $4 + 3 + 2 + 1 = 10$. 
b) In this case, only $6$. Each $A_{ij}$ must be $-A_{ji}$, but when $i=j$, $A_{ii}$ must be $0$


![[Pasted image 20220423105434.png]]

a) If $A' = A$, and $B' = B$,  is $cA' + dB'$ symmetric? We know that $cA' = cA$, and $dB' = dB$, so each term is symmetric. And we know that $(A + B)' = A' + B' = A + B$. So this is a subspace.
b) If $A' = -A$, and $B' = -B$,  is $cA' + dB'$ skew-symmetric? We know that $cA' = -cA$, and $dB' = -dB$, so each term is skew-symmetric. $(A + B)' = A' + B' = -A + -B = -(A + B)$. So it is a subspace. 
c) No. The origin must be in every subspace. 


## Practice 6

![[Pasted image 20220423112215.png]]

a) Suppose $x = s_1 + t_1$ and $y = s_2 + t_2$. Then are $cx$ and $x + y$ in $S+ T$? $cx = cs_1 + ct_1$. Since $cs_1 \in S$ and $ct_1 \in T$, $cx \in S + T$. $x + y = s_1 + s_2 + t_1 + t_2$. Since $s_1 + s_2 \in S$, and $t_1 + t_2 \in T$, $x + y \in S + T$. 

b) $S + T$ is the unique plane that contains $S$ and $T$. Consider for simplicity  $S = x_i$ and $T = x_j$ axes in $\R^m$ (which we can rotate for generality). $S \cup T$ is the set of vectors defined as   $$\{x_1, \ldots, x_m |x_p = 0 \iff p \neq  j\} \cap \{x_1, \ldots, x_m |x_p = 0 \iff p \neq  i\}$$These are the points on either axis. $S + T$ is all linear combinations of these vectors, which can give all vectors $\{(x_1, \ldots, x_m |x_p = 0 \iff p \neq (i \lor j)\}$. This is the $x_i, x_j$ plane in $\R^m$, and the span of $S \cup T$ (the set of linear combinations of vectors in $S + T$). 

![[Pasted image 20220423113740.png]]

Other points on the plane: (13,0,1); (16,1,1).
We can write $x = 3y + z + 12$. So we can say the blanks are $12,3,1$.

![[Pasted image 20220423113801.png]]

$N(A)$ is the set of $x$ such that $Ax = 0$. $Cx = \begin{bmatrix} Ax \\ Bx \end{bmatrix}$. So the null space of $C$ is $N(A) \cup N(B)$. 
## Recitation 7
The set of points such that $x - 5y + 2z = 9$ is a ___ in $\R^3$. It is ___ to the ___ $S_0$ which satisfies $x -5y + 2z = 0$. 

All points of $S$ have a specific form, 
$$\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} \_ \\ 0 \\ 0 \end{bmatrix}  + c_1\begin{bmatrix} \_ \\ 1 \\ 0 \end{bmatrix}  + c_2\begin{bmatrix} \_ \\ 0 \\ 1 \end{bmatrix} $$
---
This set of points is a plane, since it has $1$ equation and $3$ unknowns - rank $1$. It is parallel to the other plane - you can see this by writing the planes as $x = 5y-2z+9$ and $x = 5y - 2z$ (a constant shift along $x$ axis)

The blanks in the vectors are: $9, 5, -2$, as the above rewriting describes. $y$ and $z$ can be anything, but then $x$ has to be in a certain relation to them. 

NOTE: this is the same as $x - 5y + 2z = 0$ plane plus $(9,0,0)$?

Can I write this second prompt $x - 5y + 2z = 0$ as $Ax = 0$ - yes. 

$$  \begin{bmatrix}1 & -5 & 2\end{bmatrix}\begin{bmatrix} x \\ y \\ z\end{bmatrix} = 0$$
So this is just the null space of $A = (1,5,-2)$? It's a plane because $1$ dimension can be written as a linear combination of the others. If I had a rank $2$ matrix, null space would be a line.  In other words - just $1$ pivot. 

#### Revisiting
After [[ðŸš§Strang 8. Rectangular PA = LU and Ax = b]] it is clear that this is a simple case of solving $Ax =b$. We can see the null space in $S_0$, and then we shift it with a particular solution, with the free variables as zero. 

## Practice 7
![[Pasted image 20220426163829.png]]

$$ A=  \begin{bmatrix} 1 & 5 & 7 & 9 \\ 0 & 4 & 1 & 7 \\ 2 & -2 & 11 & -3\end{bmatrix}$$
$$ \begin{bmatrix} 1 & 5 & 7 & 9 \\ 0 & 4 & 1 & 7 \\ 0 & -12 & -3 & -21\end{bmatrix}$$
$$ \begin{bmatrix} 1 & 5 & 7 & 9 \\ 0 & 4 & 1 & 7 \\ 0 & 0 & 0 & 0\end{bmatrix}$$
$$ \begin{bmatrix} 1 & 0 & 23/4 & 1/4 \\ 0 & 1 & 1/4 & 7/4 \\ 0 & 0 & 0 & 0\end{bmatrix}$$

Note that this matrix has rank $2$. So we expect two special solutions. 
Suppose that $x_3 = 1$ and $x_4 = 0$. Then we have $x_1 = - 23/4$, and $x_2 = -1/4$. If $x_4 = 1$ and $x_3 = 0$ then we have $x_1 = -1/4$ and $x_2 = -7/4$. These are the entries of $-F$. 

In other words, the null space matrix takes the form $\begin{bmatrix} -F \\ I\end{bmatrix}$, or 
$$ N(A) = \begin{bmatrix} -23/4 & -1/4 \\ -1/4 & -7/4 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$$
![[Pasted image 20220426164915.png]]

$$A_1 = \begin{bmatrix} 2 & 1 \\ 1 & 2\end{bmatrix} \to A_1B = \begin{bmatrix} 3 & 3 \\ 3 & 3 \end{bmatrix}$$
$$A_2 = \begin{bmatrix} -1 & 1 \\ 1 & -1\end{bmatrix} \to A_2B =0$$
## HW 2
#### 2.5
24) 
![[Pasted image 20220426165426.png]]
$$ \begin{bmatrix} 
1 & a & b &|& 1 & 0 & 0 \\
0 & 1 & c &|& 0 & 1 & 0 \\
0 & 0 & 1 &|& 0 & 0 & 1 \\
\end{bmatrix}$$

$$ \begin{bmatrix} 
1 & a & 0 &|& 1 & 0 & -b \\
0 & 1 & 0 &|& 0 & 1 & -c \\
0 & 0 & 1 &|& 0 & 0 & 1 \\
\end{bmatrix}$$
$$ \begin{bmatrix} 
1 & 0 & 0 &|& 1 & -a & -b+ac \\
0 & 1 & 0 &|& 0 & 1 & -c \\
0 & 0 & 1 &|& 0 & 0 & 1 \\
\end{bmatrix}$$



40) 
![[Pasted image 20220426165502.png]]

$$ \begin{bmatrix}
1 & -a & 0 & 0 &|& 1 & 0 & 0 & 0\\
0 & 1 & -b & 0 &|& 0 & 1 & 0 & 0\\
0 & 0 & 1 & -c &|& 0 & 0 & 1 & 0\\
0 & 0 & 0 & 1 &|& 0 & 0 & 0 & 1\\
\end{bmatrix}$$

$$ \begin{bmatrix}
1 & 0 & 0 & 0 &|& 1 & a & ba & bac\\
0 & 1 & 0 & 0 &|& 0 & 1 & b & bc\\
0 & 0 & 1 & 0 &|& 0 & 0 & 1 & c\\
0 & 0 & 0 & 1 &|& 0 & 0 & 0 & 1\\
\end{bmatrix}$$

#### 2.6 

13) 
![[Pasted image 20220426165646.png]]

$$ E\begin{bmatrix}
a & a & a & a\\
a & b & b & b\\
a & b & c & c\\
a & b & c & d\\
\end{bmatrix} = U$$
$$ E_{,1} = 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
-1 & 1 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
-1 & 0 & 0 & 1 \\
\end{bmatrix}; 
E_{,1}A = 
\begin{bmatrix}
a & a & a & a\\
0 & b-a & b-a & b-a\\
0 & b-a & c-a & c-a\\
0 & b-a & c-a & d-a\\
\end{bmatrix}$$

$$ E_{,2} = 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & -1 & 1 & 0 \\
0 & -1 & 0 & 1 \\
\end{bmatrix}; 
E_{,2}E_{,1}A = 
\begin{bmatrix}
a & a & a & a\\
0 & b-a & b-a & b-a\\
0 & 0 & c-b & c-b\\
0 & 0 & c-b & d-b\\
\end{bmatrix}$$

$$ E_{,3} = 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & -1 & 1 \\
\end{bmatrix}; 
E_{,3}E_{,2}E_{,1}A = 
\begin{bmatrix}
a & a & a & a\\
0 & b-a & b-a & b-a\\
0 & 0 & c-b & c-b\\
0 & 0 & 0 & d-c\\
\end{bmatrix} = U$$
$$
\begin{aligned}
L &= E_{,1}\inv E_{,2}\inv E_{,3}\inv \\
& = \begin{bmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
\end{bmatrix}\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 1 & 0 & 1 \\
\end{bmatrix}\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 \\
\end{bmatrix}\\
&= \begin{bmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 1 & 1 & 0 \\
1 & 1 & 1 & 1 \\
\end{bmatrix}
\end{aligned}
$$

18) 
![[Pasted image 20220426165705.png]]
$$ \begin{aligned}
LDU &= L_1 D_1U_1 \\
L_1\inv LDU &= L_1\inv L_1 D_1U_1 \\
L_1\inv LDU U\inv&= L_1\inv L_1 D_1U_1 U\inv\\
L_1\inv LD&=  D_1U_1 U\inv\\
\end{aligned} $$
- the inverse of an U matrix is U, and L is L
- If A is U and B is U then AB is U
- So both sides must be diagonal, since they can't be L or U
- So $L_1 \inv L$ must be diagonal. and they have $1$ on diagonals. Only L diagonal matrix with $1$ on diagonals is $I$. 
- So $L_1\inv L = U_1 U \inv = I$. 


25) 
![[Pasted image 20220426165725.png]]

$$K = \begin{bmatrix}
2  & -1 & 0 &0 & 0 & 0  \\
-1 & 2 & -1 & 0 & 0 & 0 \\
0 & -1 & 2 & -1 & 0 & 0 \\
0 & 0 & -1 & 2 & -1 & 0 \\
0 & 0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & 0 & -1 & 2 \\
\end{bmatrix}$$
$$E_{2,1} = \begin{bmatrix}
1  & 0 & 0 &0 & 0 & 0  \\
1/2  & 1 & 0 &0 & 0 & 0  \\
0  & 0 & 1 &0 & 0 & 0  \\
0  & 0 & 0 &1 & 0 & 0  \\
0  & 0 & 0 &0 & 1 & 0  \\
0  & 0 & 0 &0 & 0 & 1  \\
\end{bmatrix}; E_{2,1}K = \begin{bmatrix}
2  & -1 & 0 &0 & 0 & 0  \\
0 & 3/2 & -1 & 0 & 0 & 0 \\
0 & -1 & 2 & -1 & 0 & 0 \\
0 & 0 & -1 & 2 & -1 & 0 \\
0 & 0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & 0 & -1 & 2 \\
\end{bmatrix} $$

$$E_{3,2} = \begin{bmatrix}
1  & 0 & 0 &0 & 0 & 0  \\
0  & 1 & 0 &0 & 0 & 0  \\
0  & 2/3 & 1 &0 & 0 & 0  \\
0  & 0 & 0 &1 & 0 & 0  \\
0  & 0 & 0 &0 & 1 & 0  \\
0  & 0 & 0 &0 & 0 & 1  \\
\end{bmatrix}; E_{2,1}K = \begin{bmatrix}
2  & -1 & 0 &0 & 0 & 0  \\
0 & 3/2 & -1 & 0 & 0 & 0 \\
0 & 0 & 4/3 & -1 & 0 & 0 \\
0 & 0 & -1 & 2 & -1 & 0 \\
0 & 0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & 0 & -1 & 2 \\
\end{bmatrix} $$

$$E = \begin{bmatrix}
1  & 0 & 0 &0 & 0 & 0  \\
1/2  & 1 & 0 &0 & 0 & 0  \\
1/3  & 2/3 & 1 &0 & 0 & 0  \\
1/4  & 2/4 & 3/4 &1 & 0 & 0  \\
1/5  & 2/5 & 3/5 &4/5 & 1 & 0  \\
1/6  & 2/6 & 3/6 &4/6 & 5/6 & 1  \\
\end{bmatrix}; U = \begin{bmatrix}
2  & -1 & 0 &0 & 0 & 0  \\
0 & 3/2 & -1 & 0 & 0 & 0 \\
0 & 0 & 4/3 & -1 & 0 & 0 \\
0 & 0 & 0 & 5/4 & -1 & 0 \\
0 & 0 & 0 & 0 & 6/5 & -1 \\
0 & 0 & 0 & 0 & 0 & 7/6 \\
\end{bmatrix} $$

$$E \inv = L = \begin{bmatrix}
1  & 0 & 0 &0 & 0 & 0  \\
-1/2  & 1 & 0 &0 & 0 & 0  \\
0  & -2/3 & 1 &0 & 0 & 0  \\
0  & 0 & -3/4 &1 & 0 & 0  \\
0  & 0 & 0 & -4/5 & 1 & 0  \\
0  & 0 & 0 &0 & -5/6 & 1  \\
\end{bmatrix}$$

26)
![[Pasted image 20220426165737.png]]


$$7K\inv = \begin{bmatrix}
6  & 5 & 4 &3 & 2 & 1  \\
5 & 10 & 8 & 6 & 4 & 2 \\
4 & 8 & 12 & 9 & 6 & 3 \\
3 & 6 & 9 & 12 & 8 & 4 \\
2 & 4 & 6 & 8 & 10 & 5 \\
1 & 2 & 3 & 4 & 5 & 6 \\
\end{bmatrix}$$

$$
\begin{aligned}
K*7K\inv &= \begin{bmatrix}
2  & -1 & 0 &0 & 0 & 0  \\
-1 & 2 & -1 & 0 & 0 & 0 \\
0 & -1 & 2 & -1 & 0 & 0 \\
0 & 0 & -1 & 2 & -1 & 0 \\
0 & 0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & 0 & -1 & 2 \\
\end{bmatrix}\begin{bmatrix}
6  & 5 & 4 &3 & 2 & 1  \\
5 & 10 & 8 & 6 & 4 & 2 \\
4 & 8 & 12 & 9 & 6 & 3 \\
3 & 6 & 9 & 12 & 8 & 4 \\
2 & 4 & 6 & 8 & 10 & 5 \\
1 & 2 & 3 & 4 & 5 & 6 \\
\end{bmatrix}\\
&= \begin{bmatrix} 
7 & 0 & 0 & 0 & 0 & 0 \\
0 & 7 & 0 & 0 & 0 & 0 \\
0 & 0 & 7 & 0 & 0 & 0 \\
0 & 0 & 0 & 7 & 0 & 0 \\
0 & 0 & 0 & 0 & 7 & 0 \\
0 & 0 & 0 & 0 & 0 & 7 \\
\end{bmatrix}
\end{aligned}
$$
#### 2.7

13) ![[Pasted image 20220426165808.png]]
See Practice 5.1
36) ![[Pasted image 20220426165850.png]]
- LT matrices with $1$s on the diagonal are a group. Product is obviously true - eg. $3^{rd}$ diagonal of such a product is $(x,y,1,0, \ldots)' (0,0,1, a, b, \ldots) = 1$ . Inverse?
$$A = \begin{bmatrix} 1 & 0 & 0 \\ a & 1 & 0 \\ b& c & 1\end{bmatrix}$$

$$ \begin{bmatrix} 1 & 0 & 0 &|& 1 & 0 & 0\\ a & 1 & 0 &|& 0 & 1 & 0 \\ b& c & 1 &|& 0 & 0 & 1\end{bmatrix}$$
$$ \begin{bmatrix} 
1 & 0 & 0 &|& 1 & 0 & 0\\
0 & 1 & 0 &|& -a & 1 & 0\\
0 & 0 & 1 &|& -b+ac & -c & 1\end{bmatrix}$$
Another approach: inverse of LT is LT. Only way to get $1$s on the diagonal of product is for the diagonals of inverse to be $1$. 

- Symmetric matrices. Product:
$$A = \begin{bmatrix} 1 & a & b \\ a & 1 & c \\ b& c & 1\end{bmatrix}\qquad B =  \begin{bmatrix} 1 & x & y \\ x & 1 & z \\ y& z & 1\end{bmatrix}$$
$$ AB_{1,2} = x + a + bz \qquad AB_{2,1} = a + x + cy$$
No dice, amigo

- positive matrices - no. Consider the LT matrix above
- diagonal invertible matrices. inverse - obvs true. Product - does it have to be invertible? Yes. diagonal is singular iff diagonal element is 0. multiplying two diagonals for which that is true means it's still true
- Permutation matrices. yes. Product of two permutations is obvs a permutation. Inverse is the transpose - so it is too
- $Q' = Q\inv$ matrices
What even is this? But it is a group
Let $A, B \in S$. Then  $(AB)' = B'A' = B \inv A \inv = (AB)\inv$
Inverse of $A$ is $A \inv = A'$. So $(A\inv)' = A = (A \inv)\inv$
- Identity matrix is a group
- real-valued matrices are a group

40)![[Pasted image 20220426165904.png]]

- If $Q'Q = I$ then the norms $||q_i||^2$ are precisely the diagonal elements of $Q'Q$, which are $1$. 
- Similarly, $q_i'q_j$ where $i \neq j$ gives an off-diagonal element of $I$, and therefore $0$.
$$ \begin{bmatrix} 
\cos \theta & a \\
b & c \\
\end{bmatrix}\begin{bmatrix} 
\cos \theta & b \\
a & c \\
\end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$
From this, $(\cos \theta)^2 + a^2 = 1$. So let $a = \sin \theta$. $b\cos \theta + ac = b\cos \theta + c \sin\theta = 0$, so let  $b = -\sin \theta$ and $c = \cos\theta$. Then $b^2 + c^2 = 1$ as desired.





#### 3.1

18)
![[Pasted image 20220426165925.png]]
See practice 5.3
23) 
![[Pasted image 20220426170018.png]]
unless it's linearly dependent on the others. 
example where it does: $\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}$, add column $\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}$. If you instead add $\begin{bmatrix} 1 \\1 \\ 0 \end{bmatrix}$ it does not. 

$Ax = b$ being solvable means that $b$ can be expressed as a linear combination of columns of $A$ - specifically the linear combination specified by $x$. So that means $b$ is not linearly independent of the columns of $A$, meaning the column space would not get larger. 

30) ![[Pasted image 20220426170032.png]]
See practice 6.1

32) ![[Pasted image 20220426170051.png]]
$AB$ is a linear combination of columns of $A$ (from multiplication lecture). So $AB$ is in $C(A)$. So everything that can be constructed from $AB$ can be constructed from $A$. 

blank - invertible

$$ A = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$
then rank $A$ is $1$ and rank $AA$ is $0$. 