---
date updated: 2021-09-12 17:43

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
---

## [[2001.2 Statistical Models]]
Part of [[@Stats Index]]

A **model** is an abstract representation of something, meant to highlight the most important parts for some purpose. 

#### Notation
##### Dependent or "outcome" variable
- $Y$ is $n \times 1$ matrix
- $y_i$ is a number after we know it
- $Y_i$ is a random variable (ie before we know it)

"Dependent variable": can be a whole column or a single entry 

##### Explanatory Variable
- aka "covariates", "independent", or "exogeneous"
- $X = \{x_{ij}\}$; matrix. $n \times k$, $i$ is row and $j$ is column
- Can be a set of columns $X = \{x_1 \ldots x_k \}$
- Can be a set of rows , row $i$ is  $\{x_{i1} \ldots x_{ik} \}$

By convention $X$ is usually fixed 

##### Regression Notation

Linear regression model writes $Y_i = X_i\beta + \epsilon_i$. $X_i\beta$ is the "systematic component", and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ is the "stochastic component"

##### Generalized Regression
$Y_i \sim \mathcal{N}(\mu_i, \sigma^2)$ the stochastic component
$\mu_i = x_i\beta\quad$   the systematic component

So for observation $i$, you expect it "on average" to be $x_i \beta$, but there will be normally distributed noise around this. Same as regression notation if you remember that, given constant $c$:
$$[c + \mathcal{N}(0,\sigma^2)] \sim \mathcal{N}(c,\sigma^2)$$

Question: Is a histogram of $Y$ a test of normality?
My answer: no - it includes the $X$. Imagine $X$ is $\text{Bernoulli}(.5)$ and $\sigma = .001$. Then the histogram will be two-humped even if $Y$ is drawn normally around X

General way to think about method problems: simplest possible case (he picked mine haha)

##### Generalized to a wide variety of models
$Y_i \sim f(\theta_i, \alpha)$ is the stochastic component
$\theta_i = g(x_i, \beta)$ is the systematic component. 

$Y$ is outcome variable, $f$ is a density
$\theta_i$ is some systematic feature of the density (that varies over $i$) - can be mean, variance, whatever.
$\alpha$ is an "ancillary parameter", some feature of the density that doesn't vary over $i$. For homoskedastic regression $\alpha = \sigma^2$. 
$g$ is functional form. Represents systematic component - can be linear, logistic, w/e
$x_i$ is explanatory variable vector
$\beta$ is effect parameters

In the case of regression, $\theta_i = \mu_i$, $f$ is $\mathcal{N}$, and $g$ is $g(a,b) = ab$. 

##### Forms of Uncertainty

Estimation uncertainty: we don't know $\beta$ and $\alpha$, since we haven't sampled everything in the world. Shrinks as $n$ goes up.

Fundamental uncertainty: Our model doesn't capture everything. The rest we represent in distribution $f$. This is the stochastic component

Model dependence: Maybe our whole model is wrong

Quiz - if you know the whole model including $\alpha$ and $\beta$ is $R^2$ going to be $1$?
My answer: No - fundamental uncertainty will still get you


##### Systematic Components

Systematic component can be anything. Examples: linear, logistic,
$$V(Y_i) =  \sigma_i^2 = e^{x_i\beta}$$

We're identifying _classes_ of functional forms, and then our software/math/whatever tells us which member of that class fits - by setting $\beta$

Standard procedure: 
- use theory: assume a class of functional forms to identify potential relationships
- Use data: choose which member of the class by estimating params
- Remain uncertain: fundamental & estimation uncertainty, model dependence

Wrong model -> misspecification, possibly bias
Can still get you a (linear, logistic, quadratic...) approximation of the right model
Can be close to or far from truth


##### Stochastic Components

There are different kinds: eg normal, lognormal (bounded by zero), bernoulli
poisson - discrete, countably infinite on non negatives

##### Choosing components

If one is bounded, other is too
if stochastic is between 0 and 1, systematic must be globally nonlinear or flat (possibly locally linear)

First question you ask of any empirical paper: What is the model?
