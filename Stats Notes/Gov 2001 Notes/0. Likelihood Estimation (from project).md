---
date updated: 2021-06-13 15:52

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes/stats/mle'
---

## [[0. Probability and Likelihood Estimation (from project)]]
Part of [[@Stats Index]]

### Probability Distributions

Probability distributions are models that turn parameters $p$ into values of a random variable $y$. They can tell us the probability that $y$ takes on particular values given $p$. 

For a discrete case, take a fair coin flip. This has no parameters; the probability distribution looks like this:

$$P(y|p) = f(y) = \begin{cases} \frac{1}{6}& \text{if} \: y \in \{1, \ldots, 6\}\\
\\
0 &\text{otherwise}
\end{cases} $$

The function $f$ is called a _Probability Mass Function_ (PMF). For any $x$, $f(x)$ gives the probability that $y = x$. 
 
 PMFs must only take on values between $0$ and $1$, and the sum of the PMF across all $x$ must be $1$.

### Continuous Distribution

Distributions can also be continuous. In this case, there is no PMF. Our function of interest $f(y|p)$ then is not a PMF - it is instead a _Probability Distribution Function_ (PDF). The indefinite integral of a PDF must be $1$.

In this case, $f(x)$ is a density - not interpretable as $x$, since by the properties of integration, the probability of any one $x$ is zero. 

To get probabilities, we can instead construct something like this:

$$P(x|x \in [a,b]) = \int_a^b f(x) dx$$

The area under the curve for any particular subset of $\mathbb{R}^1$ tells us the probability $x$ lies in that subset. 

### Simple Case: Bernoulli

The Bernoulli Distribution describes a situation like an unfair coin flip. It takes on values $0$ or $1$ - sometimes called failure or success. 

It has one parameter $\pi$, defined as the probability of success. So the PMF looks like this:

$$f_{bern} = \begin{cases}\pi & \text{if} \; x= 1 \\ 1-\pi & \text{if}  \; x = 0 \\
0 & \text{otherwise} \end{cases}$$

So if we flip the coin $n$ times, we can expect $100\pi$ successes and $100(1-\pi)$ failures.



### Likelihood Estimation 


Likelihood Estimation is a theory of inference

The idea is to compute likelihood - how likely is it that parameters $\theta$ generated data $y$ given model $M$, with $M$ having  $PDF_M(y,\theta)$.

We want to find the parameters $\theta$  most likely to have generated $y$ given our model.

Let's abstract away the $M$. Given parameters $\theta$, we know that $P(y|\theta) = PDF_M(y, \theta)$.  Suppse we want $P(\theta|y)$.

We can use Bayes' Theorem:

$$P(A|B) = \frac{P(A)}{P(B)}P(B|A)$$
$$P(
\theta|y) = \frac{P(\theta)}{P(y)}P(y|\theta)$$

Let $k(y)  = P(\theta)/P(y)$. Then we can say

$$P(
\theta|y) = k(y) \cdot P(y|\theta)$$



