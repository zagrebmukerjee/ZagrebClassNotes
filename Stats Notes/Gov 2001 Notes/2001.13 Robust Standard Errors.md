---
aliases: 
date updated: Tuesday, May 24th 2022, 2:28 pm

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#types/classes/stats/foundations'
- '#topics/methods'
creation date: Thursday, April 7th 2022, 3:48 pm
---

# [[2001.13 Robust Standard Errors]]
[[Gov 2001]]

High level of [[King and Roberts 2015 How Robust Standard Errors Expose Methodological Problems They Do Not Fix, and What to Do About It|King and Roberts 2015]]

[[Robust Standard Errors|Robust Standard Errors]]

### Intro

RSEs:
- aka heteroskedasticity-consistent standard errors 
- They can let you estimate $V(\thetahat)$ with fewer assumptions
- but not with NO assumptions. 
- ususally used to inoculate yrself from criticism?

When SE $\neq$ RSE
- best case
	- $\beta$s and some QOIs unbiased but inefficient
	- Some QOIs are biased (eg prob Y> .6)
- Worst case:
	- Model is misspecified and you know nothing about $V(\thetahat)$

If there's a big difference, that's an indication of misspecification


### Math
- Linear normal regression model
- In a matrix form, $Y \sim \mathcal{N}(X\beta, \Sigma)$
- In this model, $\Sigma = \sigma^2 I$ 
	- This requires assuming independence: set the off-diagonals of the VCOV matrix to 0: $\Sigma_{i,j} = 0$ if $i \neq j$.
	- Also assume homoskedasticity: $\Sigma_{i,i} = \sigma^2 \; \forall i$. So $\sigma^2 I$. 
	- note: these are separate assumptions

So what if we do regressions and $\Sigma \neq \sigma^2I$?
Deriving the formula for $\betahat$ doesn't rely on these assumptions (we're just finding the least-squares regression line/surface). So $\betahat = (X'X)^{-1}X'Y$, or $Q^{-1}X'Y$ where $Q = X'X$ . 

This is still unbiased (provably) - E$(\betahat)$ = $\beta$

An example of heteroskedasticity, one way in which this assumed $\Sigma$ can be wrong:

![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Heteroscedasticity.png/300px-Heteroscedasticity.png) 

Note that the linear regression line is the same as if these data were homoskedastic. But the variance estimate will undershoot for low X and overshoot for high X. 

The variance math:

$$\begin{align}
V(\betahat) &= V(Q^{-1}X'Y)\\
&= Q^{-1}X' V(Y)(Q^{-1}X')'\\
&= Q^{-1}X' V(Y)X''(Q^{-1})'\\
&= Q^{-1}X' V(Y)XQ^{-1}\text{ because } Q^{-1} \text{ is symmetric} \\
&=Q^{-1}X' \Sigma XQ^{-1}
\end{align}
$$

This is not $\sigma^2I$ unless $\Sigma = \sigma^2 I$. So  - biased. All sims are biased, all uncertainty ests are biased. 

### The RSE trick

$\Sigma$ above is way too hard to estimate, $n \times n$. We only have $n$ observations... 

But $X'\Sigma X$ only has $k \times k$ parameters, which we can definitely estimate (since $k$ is usually less than $n$).

It turns out that we can estimate $\sigma^2_i$ with $e^2_i$ (the residual)
We are still assuming independence, but this accomodates heteroskedasticity (imagine the residuals on the cone-shaped scatter above - not a terrible estimate). Even if the DGP is heteroskedastic, the RSE will be correct. 

This is the RSE. It's generalizable to any MLE. 

But - the model could still be misspecified! If it's very misspecified, your $\betahat$ will still be wrong. So RSEs only help if model is misspecified in a very specific way. And you still can't rely on it to get some QOIs (why?) And if your homoskedasticity assumption is wrong - maybe the rest of them are...

If RSE $\neq$ SE: find the misspecification and fix the model. 
