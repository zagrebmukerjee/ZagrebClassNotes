---
date updated: 2021-09-19 12:11

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classes/stats/foundations'
---

## [[2001.4 Probability]]

### Probability 
Probability is a model of the world

A function Pr$(y |M)$, probability of the data given the model
Model has $f, g, X, \beta, \alpha$ 
We can omit $M$ and just write Pr$(y)$

Probability axioms:
1. Given event $z$, Pr$(z) \geq 0$
2. Probability of sample space = 1
3. if $z_1, \ldots z_k$ are mutually exclusive then Pr$(z_1 \cup  \ldots \cup  z_n) =$ Pr$(z_1) + \ldots +$ Pr$(z_n)$

Above implies Pr is bounded above by 1

Probabilities are described by density: for discrete probs/PMF, sums =1, for continuous probs/PDF, integral = 1. Density is defined for all possible $y$ or range of $y$ - all possible outcomes.

For continuous - Pr$(y) = 0$ for any $y$, but probabilities are defined over intervals.  
##### Variance:

Variance is a measure of dispersion - the  squared expected value of distance from the expected value. 
$$\begin{align}V(Y) &= E[(Y - E(Y))^2]\\ &= E(Y^2) - E(Y)^2\end{align}$$

How to get $E(Y^2)$? Special case of $E(g(y))$. We can get that with 
$$E(g(y)) = \sum_y g(y)\text{P}(y)$$



### Example Distributions

##### Bernoulli

What is Bernoulli distribution: $Y$ is either $1$ or $0$. Parameter is $\pi_i$.  Pr$(Y_i = 1|\pi_i) = \pi_i$ and by definition Pr$(Y_i = 0|\pi_i) = 1- \pi_i$. Can also write Pr$(Y_i = y| \pi_i) = \pi_i^y(1-\pi_i)^{1-y}$

Expected Value: in general sense is $E(Y) = \sum_y yP(y)$. Here that's $\pi$
Variance: $E(Y^2) - E(Y)^2 = E(Y^2) - \pi^2 = \pi - \pi^2 = \pi(1-\pi)$

##### Binomial 

$N$ i.i.d. Bernoullis, $y_1, \ldots, y_n$. We observe $Y$, the sum. 
$$\binom{N}{y} \pi^y(1-\pi)^{N-y}$$

##### Uniform

every interval is same probability

##### Normal

Commonly used: eg central limit theorem
PDF is 
$$(2\pi\sigma^2)^{-1/2}\exp \left(\frac{-(y - \mu)^2}{2\sigma^2} \right)$$

##### MV Normal
Can give you $Y_i = \{Y_{1i}, \ldots, Y_{ki}\}$ which is "jointly random"

We have $\mu$ as a $k\times 1$ vector, and $\Sigma$ as $k \times k$

### Simulation

Can draw from arbitrary distributions in two ways:
- Discretization: trapezoidal approx to the integral of whatever function. Infeasible for many distributions
- Inverse CDF method: compute the CDF - the numerical integral. Create an inverse functio $F^{-1}$. Then $F^{-1}(U)$ is a random draw of $f(y)$
- Combine the two: refined discretization. choose interval in a trapezoid and use inverse CDF. 

A huge literature on this





