---
date updated: 2021-10-03 17:42

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#types/classes/stats/foundations'
- '#types/classes/stats/mle'
---

# [[2001.7 Uncertainty in Likelihood Estimation]]


How to describe the uncertainty in your estimates?


Three ways:
1. likelihood ratio: the difference between two levels of likelihood function is the likelihood ratio - generalization of the f test
2. Standard Error: the curvature of the likelihood function at the MLE
3. Rao's score (not really a focus here) - slope of the likelihood function at a given parameter. Based on just one point 

## Likelihood Ratio

- How can we compare two likelihood models? 
	- Consider 2 models: 1 is unrestricted, has MLE $L^*$
	- other model is restricted, $L^*_R$ i.e. some parameters omitted
	- We know that $L^* \geq L^*_R$ so $\frac{L^*_R}{L^*} \geq 1$
	- Since $k(y)$ is the same \[ same dataset\] we can write that ratio as 
	$$\frac{L(\theta_1 | y)}{L(\theta_2 | y)}  = \frac{P(y | \theta_1)}{P(y | \theta_2)}$$
	- We can use this to do Neyman-Pearson hypothesis testing on whether the restriction $R$ matters (vs the null that it doesn't)


## Standard Errors

Summarizes the likelihood curvature near the max with one number - or a $k \times k$ matrix of numbers if more covariates

- Use the normal likelihood to approximate all likelihoods (CLT tells us that all likelihoods become normal as $n \to \infty$. )
- Example: the normal with $\mu_i = \beta$ and $\sigma^2$
- The normal log likelihood  becomes: $$ -\frac{n}{2}\ln(2\pi \sigma^2) + \frac{\sum y_i}{\sigma^2}\beta + \frac{-n}{2\sigma^2}\beta^2$$
- That last coefficient $\frac{-n}{2\sigma^2}$ represents the curvature of this quadratic function. A more curved likelihood means more certainty around the estimate. 
	
#### Generalized

The curvature is the quadratic term on the quadratic approximation to this likelihood:
$$ \frac{\partial^2 \ln L(\beta_y)}{\partial \beta^2} = \frac{-n}{\sigma^2}$$

The statistical interpretation of negative inverse curvature, evaluated at $\thetahat$  is $\hat{V}(\thetahat)$. This is the variance-covariance matrix. What is the first element in the matrix, $\hat{\sigma}_1$? It's the amount that $\theta_1$ would vary in repeated samplings from this population


square roots of the diagonal of the inverse of the hessian