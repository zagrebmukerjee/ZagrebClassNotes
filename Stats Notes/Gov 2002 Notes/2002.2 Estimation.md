---
date updated: 2022-02-20 18:49

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes'
- '#ðŸš§'
---

# [[2002.2 Estimation]]
Part of [[@Stats Index]]


## Point Estimation

Inference is an attempt to guess about the properties of a population given a sample. 

An _estimator_ is a function of sample data. We use it to learn about _estimands_. Using an estimator, you can create _estimates_. 


This can get pretty theoretical. For example, an experiment can be conceived as sampling from a world in which everyone is treated. 

### Setup

Assume a population with an unknown distribution, mean $\mu$ and variance $\sigma^2$ .

A sample gives you observations $Y_1, \ldots, Y_n$. If the sample is truly random, then the observations are i.i.d. (a key assumption). We can write the sample mean as $\bar{Y}.$

In this case $E(Y_i) = \mu$, and $V(Y_i) = \sigma^2$

Imagine drawing samples over and over, and applying an estimator to each sample. This would create a distribution of estimates, known as that estimator's _sampling distribution_. In reality, we probably have only one sample - so we have to make inferences about the sampling distribution. 


### Finite-Sample Properties of Estimators

We want an estimator to be _unbiased_: $E[\muhat] = \mu$ over repeated samples. 

We also prefer estimators that are more _efficient_. $\muhat_1$ is more efficient than $\muhat_2$ if $V(\muhat_1) < V(\muhat_2)$ - this being the variance of the sampling distribution. 

The square root of the variance of the sampling distribution is the _standard error_. 


##### Relative Estimator Quality

One way to measure the tradeoff between bias and error is the mean squared error. 

$$MSE(\thetahat) = E[\thetahat - \theta] = \text{Bias}(\thetahat)^2 + V(\thetahat) $$





##### Estimators of variance

Is the 'Plug-in sample of variance', $\overline{X^2} - \bar{X}^2$, an unbiased estimator of population variance?

Let $Z = X^2$. Then $E[\overline{X^2}] = E[\bar{Z}] = E[Z] = E[X^2]$. So far so good. 

We can write 
$$
\begin{aligned}
E[\bar{X}^2] &= E[E(\bar{X})^2  + \bar{X}^2 - E(\bar{X})^2]\\
&= E(\bar{X})^2  + E(\bar{X}^2) - E(\bar{X})^2 \\ 
&= E(\bar{X})^2 + V(\bar{X}) \\
&= E(\bar{X})^2 + \sigma^2/n \\
\end{aligned}
$$

So what is $E[\overline{X^2} - \bar{X}^2]$?

$$
\begin{aligned}
E[\overline{X^2} - \bar{X}^2] &= E(\overline{X^2}) - E(\bar{X}^2) \\
&= E[X^2] -  E(\bar{X})^2 +-\sigma^2/n  \\
&= E[X^2] -  E(X)^2 - \sigma^2/n  \\
&= \sigma^2 - \sigma^2/n \\
&= \frac{n-1}{n} \sigma^2
\end{aligned}
$$

Hence we use a correction, the sample variance - 

$$\hat{V}(X) = \hat{\sigma}^2 = \frac{n}{n-1} [\overline{X^2} - \bar{X}^2]$$

### Asymptotic Properties of Estimators

Consider the behavior of $\thetahat_1, \ldots \thetahat_n$, increasing in sample size. How does this sequence behave?

If $\exists \lim_{n \to \infty} \thetahat_n$ then we have 'stochastic convergence'

There are two types of convergence:
- Convergence in probability: $X_n$ converges in probability to $a$, or $X_n \xrightarrow{p} a$ if 

$$ \lim_{n \to \infty} P(|X_n - a| < \varepsilon) \; \forall \; \varepsilon > 0$$

A sufficient but unnecessary criterion: $E(X_n) \to a$, $V(X_n) \to 0$ (collapse to a spike)
Ex. sample mean converges in probability to population mean.



##### Consistency

An estimator is consistent if $\thetahat_1, \ldots, \thetahat_n$ converges in probability to true value $\theta$. 

A pretty minimal requirement. No guarantee of finite sample performance. Not the same as unbiasedness - e.g. $\bar{X} + 5$ is biased but consistent. 

##### Weak Law of Large Numbers 
If $X_1, \ldots, X_n$ are i.i.d sequence of RV, each with finite mean $\mu$, then $\bar{X}_n \xrightarrow{p} \mu$ as $n \to \infty$. 

In other words, the sample mean converges in probability to the expected value as sample size grows. 



##### Central Limit Theorem and Asymptotic Normality

A very good theorem.

Allows us to characterize the sampling distriubtion: 

- Convergence in distribution: limit of the CDFs of $X_n$ is equal to CDF of $A$ at every point where the CDF of $A$ is continuous. We write this as $X_n \xrightarrow{d} A$. Convergence in probability is a special case of convergence in distribution where $V(A) = 0$. 


Now we can understand the Central Limit Theorem: 

Let $X_1, \ldots, X_n$ be a sequence of i.i.d. RVs with finite mean $\mu$ and variance $\sigma^2$. Then, regardless of how $X_i$ are distributed: 
$$\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} \mathcal{N}(0,\sigma^2)$$

In other words, as the sample size grows, the sampling distribution becomes more normal around the population mean. 	

The CLT also means that standardized sample mean - z-score - converges to standard normal.