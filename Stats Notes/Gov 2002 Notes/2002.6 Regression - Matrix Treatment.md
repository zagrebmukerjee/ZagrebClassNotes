---
date updated: 2022-04-09 17:37

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes/stats/regression'
---

# [[2002.6 Regression - Matrix Treatment]]
Now it gets good

- [[#Matrix Notation for Regressions|Matrix Notation for Regressions]]
- [[#Regression Estimator|Regression Estimator]]
- [[#Gauss-Markov Redux|Gauss-Markov Redux]]
	- [[#Gauss-Markov Redux#Unbiased|Unbiased]]
	- [[#Gauss-Markov Redux#Variance|Variance]]
- [[#Hypothesis testing|Hypothesis testing]]
	- [[#Hypothesis testing#Joint Hypothesis Testing|Joint Hypothesis Testing]]
		- [[#Joint Hypothesis Testing#Example of joint hypothesis test|Example of joint hypothesis test]]
		- [[#Joint Hypothesis Testing#F Test Example 2|F Test Example 2]]
		- [[#Joint Hypothesis Testing#Wald test and Generalized restriction model|Wald test and Generalized restriction model]]


## Matrix Notation for Regressions

We can write a regression model in matrix form as follows:

$$Y = X\beta + u$$
Here, $Y$ is an $N \times 1$ vector of outcomes. $X$ is an $N \times k$ vector of covariates. $u$ is a $N \times 1$ vector of errors. 

We approximate this with 

$$ Y = X\betahat + \uhat$$
OK, so what does this look like inside? Let's do a small example, with $n =5$ and $k=3$:

$$\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\y_4  \end{bmatrix} = \begin{bmatrix}
x_{1,1} & x_{1,2} & x_{1,3} \\
x_{2,1} & x_{2,2} & x_{2,3} \\
x_{3,1} & x_{3,2} & x_{3,3} \\
x_{4,1} & x_{4,2} & x_{4,3} \\
x_{5,1} & x_{5,2} & x_{5,3} \\
\end{bmatrix} \begin{bmatrix} \betahat_1\\ \betahat_2 \\ \betahat_3 \end{bmatrix} + \begin{bmatrix} \uhat_1 \\ \uhat_2 \\ \uhat_3 \end{bmatrix}$$

Let's multiply out only the first row of $X\beta$: 
$$y_1 - \hat{u}_1= \begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,3} \end{bmatrix}\begin{bmatrix} \betahat_1\\ \betahat_2 \\ \betahat_3 \end{bmatrix} =  \betahat_1 x_{1,1} + \betahat_2 x_{1,2} + \betahat_3 x_{1,3} $$

We're going to end up with a very familiar system of linear equations:  $y_i = \betahat_1 x_{1,i} + \betahat_2 x_{2,i} + \betahat_3 x_{3,i} + \uhat_i$ - three covariates, three coefficients. Note here that the *row* indicators for $X$ became *columns* of our system. Each *column* of $X$ represents $n$ observations of some particular covariate

If you're worried where the intercept went, worry not. $x_{i,1}$ is usually $1$ by convention, which makes $\betahat_1$ our constant term.

## Regression Estimator

Three important matrix differentiation results: 
1. $\frac{d}{dx} a'x = \frac{d}{dx} x'a= a$
2. $\frac{d}{dx'}a'x = a'$
3. $\frac{d}{dx} x'Ax = 2Ax$. (called the quadratic form)

See [[Matrix Calculus]] for detail; and recall [[Matrix Properties]].

We want to minimize the sum of $\uhat_i$. Fortunately, we can represent that with $\uhat \cdot \uhat$ - a dot product - or equivalently $\uhat '\uhat$. So: 

$$ 
\begin{aligned}
\uhat' \uhat &= (y - X\betahat)'(y - X\betahat)\\
&= (y' - \betahat'X')(y - X\betahat)\\
&= y'y - 2y'X\betahat - \betahat'X'X\betahat\\
\end{aligned}
$$
Now we can start using the matrix-calculus results to differentiate the minimand with respect to $\betahat$. The first term drops away. For the second term, note that $y \times X$ is a $1 \times k$ vector, so we can use the first result: the derivative is $-X'y$. The third term: $X'X$ is symmetric. So this is the quadratic form, with $A = X'X$ (result 3): $2X'X\betahat$.

This gives us the first-order condition to solve:

$$
\begin{aligned}
0 = -2X'y - 2X'X \betahat\\
\betahat = (X'X)^{-1}X'y \\
\end{aligned}
$$
The requirement that $X'X$ be invertible means that $X$ must be invertible - i.e. that it have no collinearity.

## Gauss-Markov Redux

We can restate the regression assumptions:
- Linearity: $y = X\beta + u$
- Random Sampling
- No perfect collinearity - $X$ is full (column) rank (that is, all columns are linearly independent of one another). This is true iff $X$ is nonsingular
- Zero Conditional Mean: $E[u|X] = 0$, meaning $E[u] = 0$ and $\cov(u_i, X) = 0$ for all $u_i$
- Homoskedasticity $\var(u|X)  = \sigma^2 I$
- Bonus assumption: Normality of errors: $u|X \sim \mathcal{N}(0, \sigma^2 I)$  (this being the multivariate normal), implying $Y|X \sim \mathcal{N}(X\beta, \sigma^2 I)$. This should be familiar as a model taught in 2001, where $X\beta$ is what we called a systematic component, and $u$ is the stochastic component. [[2001.2 Statistical Models]]. We don't need this for Gauss-Markov, but it gets us small-sample performance.

### Unbiased
Demonstrating $\betahat$ is unbiased requires assumptions 1-4:

$$
\begin{aligned}
E[\betahat] &= E[(X'X)^{-1}X'y] \\
&= E[(X'X)^{-1} X'(X\beta + u)] \\
&= E[(X'X)^{-1} (X'X\beta + X'u)] \\
&= \beta E[(X'X)^{-1} X'X] + E[(X'X)^{-1} X'u] \\
&= \beta I + E[(X'X)^{-1} X'u] \\
E[\betahat|X] &= \beta + E[(X'X)^{-1} X'u |X]\\
&= \beta + (X'X)^{-1}X' E[u |X] \\ 
&= \beta
\end{aligned}
$$

### Variance

Variance of $\betahat$: 
Let $Q = X'X$, a symmetric invertible matrix. Then
$$
\begin{aligned}
\var(\betahat|X) &= \var[Q^{-1}X'y |X]\\
&= \var[Q^{-1}X'(X\beta + u)|X] \\
&= Q^{-1}X'\var[X\beta + u] XQ^{-1} \\
&= Q^{-1}X'[\var(X\beta|X) + \var(u|X) + 2 \cov(X\beta,u|X)] XQ^{-1}\\
&= Q^{-1}X'[\var(u)]XQ^{-1} \\
\end{aligned}
$$
With the homoskedasticity assumption, we get a variance-covariance matrix:

$$
\begin{aligned}
\var(\betahat|X) &= Q^{-1}X'[\var(u)]XQ^{-1}\\
&= Q^{-1}X'\sigma^2IXQ^{-1}\\
&= \sigma^2[Q^{-1}X'XQ^{-1}]\\
&= \sigma^2[(X'X)^{-1}]\\
\end{aligned}
$$
Note here: The dimensions of $X$ are $n \times k$. $X'X$ has dimensions $(k \times n)(n \times k) = k \times k$; inverted, the same. 

Our estimate of the variance $\sigma$ comes from $\hat\sigma^2$, the unbiased estimator, which is:

$$\hat\sigma^2 = \frac{SSR}{n  - k - 1} = \frac{u'u}{n-k-1}$$ 
We can demonstrate consistency by using the sample means and using the LLN. 

Note: $X'X = \sum x_i'x_i$ and $X'Y = \sum x_i' y_i$ where $x_i$ is a $1 \times k$ vector that's the $i^{th}$ row of $X$. 


## Hypothesis testing

Inexplicably, here we are again. This Neyman has a lot to answer for

The standard errors of the vector $\betahat$ are the $1 \times k$ vector of square roots of the diagonals of the variance -covariance matrix. In math, 
$$SE(\betahat) = \sqrt{Vcov(\betahat)_{i,i}} =  \sqrt{\hat{\sigma} (X'X)^{-1}_{i,i}}$$
We can then make any given cooefficient's $t$ values by comparing the $i^{th}$ standard error and  $\betahat_i$ to the appropriate null hypothesis. Use the $t$ distribution with degrees of freedom $n - k - 1$. Same deal for the confidence intervals.. 

For two coefficients at a time: recall that difference in estimators test is 

$$\frac{\betahat_1 - \betahat_2}{\sqrt{\var(\betahat_1 - \betahat_2)}} = \frac{\betahat_1 - \betahat_2}{\sqrt{\var(\betahat_1) + \var(\betahat_2) -2\cov(\betahat_1, \betahat_2)}}$$
The variance-covariance matrix's off-diagonal elements are covariances - so the term $\cov(\betahat_1, \betahat_2)$ is the first row, second column of the matrix.


### Joint Hypothesis Testing

If I want to test a joint hypothesis - that $\betahat_1$ and $\betahat_2$ are, taken together, distinguished from the hypothesis that both are nulls -  my test statistic should be compared to a different distribution, the $\chi^2$ distribution. This describes the distribution of the sum of squares of normally distributed RVs: if $Z_1, \ldots, Z_n$ are i.i.d standard normals, then $X = \sum Z_i^2 \sim \chi^2_n$. $E[X] = n, \var(X) = 2n$.

Now, suppose $X_1 \indep X_2$, where $X_1 \sim \chi^2_{df_1}$ and $X_2 \sim \chi^2_{df_2}$. Then we can draw on the F distribution.

$$ F = \frac{X_1/df_1}{X_2/df_2} \sim \mathcal{F}_{df_1, df_2}$$
#### Example of joint hypothesis test

Suppose we want to determine whether women vote at the same rate as men. We use the following model: 

$$\text{Voting} = \beta_0 + \gamma_1 I_\text{Female} + \beta_1 \text{Education} + \beta_2 \text{Age} + \gamma_2(\text{Female} \times \text{Education})$$
$$ + \gamma_3(\text{Female} \times \text{Age}) + u$$
The null hypothesis here is that $\gamma_1 = \gamma_2 = \gamma_3 = 0$; that is to say, being female has no effect on voting, neither directly nor through changing the education or age effects. 

So we want to know: Are all the $\gamma$ close to $0$, statistically speaking? What about the combination of $\gamma$? If both, then we fail to reject the null. If we reject the joint null, that doesn't mean we can reject any of the nulls $\gamma_i = 0$, or vice versa.

We can think of this as the difference between two models: a *restricted* model, estimated as if $H_0$ were true, and an *unrestricted* model, estimated as normal. To compare these models, we want to make the <font color=gree>F statistic</font>. 

We first fit an unrestricted model: the model above. Then we fit another model, where $H_0$ is true, so $\text{Voting}$ $\sim \beta_0$ $+ \beta_1 \text{Education} + \beta_2 \text{Age}$.

Then the F-statistic is the relative increase in (minimized) SSR going from unrestricted to restricted. I.e. how much more of the outcome is explained when $H_0$ is allowed to be false. Specifically, 

$$F_0 = \frac{(SSR_R - SSR_{UR})/q}{SSR_{UR}/(n - k - 1)}$$

Here, $SSR_R$ is the sum-squared residuals from the restricted model, $SSR_{UR}$ from the unrestricted model, $q$ is the number of restrictions (3), $k$ is the number of UR predictors. 

$F_0$ is then distributed according to $\mathcal{F}_{q, n-k-1}$. 

With normal errors we can use the F-test at whatever size - without that assumption we can only use it asymptotically.

One commonly used $F$ test is the omnibus test. Test if your coefficients (excepting the constant) can reject the null hypothesis that they're all zero. Is my fancy regression any better than $\ybar$? 

#### F Test Example 2

Now, we might be interested in differences. Suppose $Y = X\beta+ u$, and we hypothesize $H_0$: $\beta_1 = \beta_2 = \beta_3$. 

This leads to two restrictions, $\beta_1 - \beta_2 = 0$, and $\beta_2 - \beta_3 = 0$. 

The unrestricted model here is $Y = X\beta + u$. The restricted model is $Y = \beta_0 + \beta_1(X_1 + X_2 + X_3) + \ldots + \beta_kX_k$. 

Then we can proceed as before. 

In the case when we are testing $\beta_1 = \beta_2$, we can either do the difference-in-coefficients $t$ test, or the $F$ test with restriction $\beta_1 - \beta_2 =0$. This is because $q=1$: 
$$ X \sim t_{n-k-1} \iff X^2 \sim F_{1,n-k-1}$$
```ad-warning

Jointly significant does not mean any particular one is significant. Nor vice versa. 

```

#### Wald test and Generalized restriction model
More generally, we can say that any set of restrictions takes the form $R\beta = r$, where $R$ is $q \times k$, and $r$ is a vector of $q$ constants. 

So consider the $H_0$: $R\beta = r$, so $R\beta-r = 0$. 

We can use this to create a thing called the Wald statistic with known distribution:

$$W = (R\betahat-r)'[\hat{\sigma}^2R(X'X)^{-1}R' ]^{-1}(R\betahat-r)$$
First and last components represent sum of squares of $R\betahat-r$. How far are the restrictions from $0$? Middle component is the variance of $R\betahat-r$, standardizing the sum of squares. Since $\betahat$ is approximately normal, then $R\betahat-r$ is normal, then $W$ is $\chi^2$ distributed. We need assumptions $1-5$ for asymptotic normality (which implies $W \sim \chi^2_q$ in the limit) With assumption $6$, we can say for any sample size that $W/q \sim \mathcal{F}_{q, n-k-1}$. Which converges to the other.

In general people use the F distribution for the Wald statistic, for conservatism. 

