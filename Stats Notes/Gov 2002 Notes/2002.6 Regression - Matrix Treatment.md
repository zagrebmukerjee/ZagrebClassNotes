---
date updated: 2022-04-09 17:37

notetype: "Math Class Note"
cssclass: math-class-note

tags: 
- '#classnotes/stats/regression'
---

# [[2002.6 Regression - Matrix Treatment]]
Now it gets good

## Matrix Notation for Regressions

We can write a regression model in matrix form as follows:

$$Y = X\beta + u$$
Here, $Y$ is an $N \times 1$ vector of outcomes. $X$ is an $N \times k$ vector of covariates. $u$ is a $N \times 1$ vector of errors. 

We approximate this with 

$$ Y = X\betahat + \uhat$$
OK, so what does this look like inside? Let's do a small example, with $n =5$ and $k=3$:

$$\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\y_4  \end{bmatrix} = \begin{bmatrix}
x_{1,1} & x_{1,2} & x_{1,3} \\
x_{2,1} & x_{2,2} & x_{2,3} \\
x_{3,1} & x_{3,2} & x_{3,3} \\
x_{4,1} & x_{4,2} & x_{4,3} \\
x_{5,1} & x_{5,2} & x_{5,3} \\
\end{bmatrix} \begin{bmatrix} \betahat_1\\ \betahat_2 \\ \betahat_3 \end{bmatrix} + \begin{bmatrix} \uhat_1 \\ \uhat_2 \\ \uhat_3 \end{bmatrix}$$

Let's multiply out only the first row of $X\beta$: 
$$\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,3} \end{bmatrix}\begin{bmatrix} \betahat_1\\ \betahat_2 \\ \betahat_3 \end{bmatrix} =  \betahat_1 x_{1,1} + \betahat_2 x_{1,2} + \betahat_3 x_{1,3} $$

We're going to end up with a very familiar system of linear equations:  $y_i = \betahat_1 x_{1,i} + \betahat_2 x_{2,i} + \betahat_3 x_{3,i} + \uhat_i$. Note here that the *row* indicators for $X$ became *columns* of our system.

If you're worried where the intercept went, worry not. $x_{i,1}$ is usually $1$ by convention, which makes $\betahat_1$ our constant term.

## Regression Estimator

Three important matrix differentiation results: 
1. $\frac{d}{dx} a'x = \frac{d}{dx} x'a= a$
2. $\frac{d}{dx'}a'x = a'$
3. $\frac{d}{dx} x'Ax = 2Ax$. 

See [[Matrix Calculus]] for detail; and recall [[Matrix Properties]].

We want to minimize the sum of $\uhat_i$. Fortunately, we can represent that with $\uhat \cdot \uhat$ - a dot product - or equivalently $\uhat '\uhat$. So: 

$$ 
\begin{aligned}
\uhat' \uhat &= (y - X\betahat)'(y - X\betahat)\\
&= (y' - \betahat'X')(y - X\betahat)\\
&= y'y - 2y'X\betahat - \betahat'X'X\betahat\\
\end{aligned}
$$
Now we can start using the matrix-calculus results to differentiate the minimand with respect to $\betahat$. The first term drops away. For the second term, note that $y \times X$ is a $1 \times k$ vector, so we can use the first result: the derivative is $-X'y$. The third term: $X'X$ is symmetric. So this is the quadratic form (result 3): $2X'X\betahat$.

This gives us the first-order condition to solve:

$$
\begin{aligned}
0 = -2X'y - 2X'X \betahat\\
\betahat = (X'X)^{-1}X'y \\
\end{aligned}
$$
The requirement that $X'X$ be invertible means that $X$ must be invertible - i.e. that it have no collinearity.

## Gauss-Jordan Redux

We can restate the regression assumptions:
- Linearity: $y = X\beta + u$
- Random Sampling
- No perfect collinearity - $X$ is full rank (that is, all columns are linearly independent of one another). This is true iff $X$ is nonsingular
- Zero Conditional Mean: $E[u|X] = 0$, meaning $E[u] = 0$ and $\cov(u_i, X) = 0$ for all $u_i$
- Homoskedasticity $\var(u|X)  = \sigma^2 I$
- Bonus assumption: Normality of errors: $u|X \sim \mathcal{N}(0, \sigma^2 I)$  (this being the multivariate normal), implying $Y|X \sim \mathcal{N}(X\beta, \sigma^2 I)$. This should be familiar as a model taught in 2001, where $X\beta$ is what we called a systematic component, and $u$ is the stochastic component. [[2001.2 Statistical Models]]


Demonstrating $\betahat$ is unbiased:

$$
\begin{aligned}
E[\betahat] &= E[(X'X)^{-1}X'y] \\
&= E[(X'X)^{-1} X'(X\beta + u)] \\
&= E[(X'X)^{-1} (X'X\beta + X'u)] \\
&= \beta E[(X'X)^{-1} X'X] + E[(X'X)^{-1} X'u] \\
&= \beta I + E[(X'X)^{-1} X'u] \\
E[\betahat|X] &= \beta + E[(X'X)^{-1} X'u |X]\\
&= \beta + (X'X)^{-1}X' E[u |X] \\ 
&= \beta
\end{aligned}
$$



Variance of $\betahat$: 
Let $Q = X'X$, a symmetric invertible matrix. Then
$$
\begin{aligned}
\var(\betahat|X) &= \var[Q^{-1}X'y |X]\\
&= \var[Q^{-1}X'(X\beta + u)|X] \\
&= Q^{-1}X'\var[X\beta + u] XQ^{-1} \\
&= Q^{-1}X'[\var(X\beta|X) + \var(u|X) + 2 \cov(X\beta,u|X)] XQ^{-1}\\
&= Q^{-1}X'[\var(u)]XQ^{-1} \\
\end{aligned}
$$
With the homoskedasticity assumption, we get 

$$
\begin{aligned}
\var(\betahat|X) &= Q^{-1}X'[\var(u)]XQ^{-1}\\
&= Q^{-1}X'\sigma^2IXQ^{-1}\\
&= \sigma^2[Q^{-1}X'XQ^{-1}]\\
&= \sigma^2[(X'X)^{-1}]\\
\end{aligned}
$$


## Hypothesis testing

Inexplicably, here we are again. This Neyman has a lot to answer for

The standard errors of the vector $\betahat$ are the $1 \times k$ vector of square roots of the diagonals of the variance -covariance matrix. In math, 
$$SE(\betahat) = \sqrt{Vcov(\betahat)_{i,i}} =  \sqrt{\hat{\sigma} (X'X)^{-1}_{i,i}}$$
We can then make a vector of $t$ values by comparing each standard error and each $\betahat_i$ to the appropriate null hypothesis. Probably some can be two tailed and some not. 